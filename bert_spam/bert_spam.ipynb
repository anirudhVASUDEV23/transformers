{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5dd3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f26dffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, multiprocess, datasets\n",
      "Successfully installed datasets-4.0.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15eef748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef811aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"spam.csv\")\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001a372",
   "metadata": {},
   "source": [
    "### Category is text we'll convert it into a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9635e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category']=df['Category'].map({'ham':0,'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f08b8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "499b01c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    4825\n",
       "1     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83a678",
   "metadata": {},
   "source": [
    "### Reducing the dataset size to reduce training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b061bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam=df[df['Category']==1]\n",
    "df_spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be7d7630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ham_small=df[df['Category']==0].sample(n=1000, random_state=42)\n",
    "df_ham_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d1f82a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small=pd.concat([df_spam,df_ham_small])\n",
    "df_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bc6ba25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    1000\n",
       "1     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69c39d",
   "metadata": {},
   "source": [
    "### Train,Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66fc381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(df_small['Message'],df_small['Category'],test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60bffb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1397,), (350,), (1397,), (350,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bbb6a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    812\n",
       "1    585\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "169909ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    188\n",
       "1    162\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596346c",
   "metadata": {},
   "source": [
    "### Tokenizing the Messages we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be7b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  7592,  1010,  2129,  2024,  2017,  1029,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2023,  2003,  1037, 12403,  2213,  4471,  1012,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([0., 1.]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(texts,labels):\n",
    "    encodings=tokenizer(texts,padding='max_length',max_length=128,truncation=True,return_tensors='pt')\n",
    "    return encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels,dtype=torch.float)\n",
    "\n",
    "tokenize_function([\"Hello, how are you?\", \"This is a spam message.\"], [0, 1])##each token dimension is 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df01ebf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986ab74",
   "metadata": {},
   "source": [
    "### Different ways of converting X_train to list so that we can pass it into the tokenize_function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3e8d951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes:)sura in sun tv.:)lol.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checking=X_train.tolist()\n",
    "checking[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a89332f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes:)sura in sun tv.:)lol.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values[3] ##converts the pandas series to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a58a9b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd535666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train.values.tolist()) ## converts the pandas series to list via numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "497f6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_masks, train_labels=tokenize_function(X_train.values.tolist(), y_train.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9cc4b5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_input_ids), type(train_attention_masks), type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5f2f975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101, 22950,  1011,  4471,  2415,  1009,  4008,  2581,  2581,  2683,\n",
       "         2581, 19841, 16086,  2692,  2683,  2339,  3524,  1029,  6611,  2005,\n",
       "         2115,  2925,  8299,  1024,  1013,  1013, 10922,  1012, 22950,  1012,\n",
       "         4012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af8eee4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e4f9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input_ids, val_attention_masks, val_labels=tokenize_function(X_test.values.tolist(), y_test.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f988bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "test_dataset=torch.utils.data.TensorDataset(val_input_ids, val_attention_masks, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7a3ae234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first part represents the input_ids of the sentence: torch.Size([128])\n",
      "The second part represents the attention mask: torch.Size([128])\n",
      "The third part represents the label: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first part represents the input_ids of the sentence: {train_dataset[0][0].shape}\")\n",
    "print(f\"The second part represents the attention mask: {train_dataset[0][1].shape}\")\n",
    "print(f\"The third part represents the label: {train_dataset[0][2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f857cef",
   "metadata": {},
   "source": [
    "#### ok,now all these tokens are passed to bert model which gives out contextual embeddings right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f51b4d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape ##each word is represented by 128 tokens with the first token being the [CLS] token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63234438",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849e907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert= BertModel.from_pretrained('bert-base-uncased')\n",
    "bert.to(device)\n",
    "bert.config.hidden_size ## hidden size of the BERT model, which is 768 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "326b7d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e537503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False ##Freezing all BERT layers\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 256),##768 is the hidden size of BERT\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output=self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embedding=bert_output.last_hidden_state[:,0,:]## Means that for each sentence,take the CLS token output and take all the 768 dimensions of the CLS token output\n",
    "        return self.classifier(sentence_embedding)##passing the CLS token output through the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ce44b",
   "metadata": {},
   "source": [
    "#### No, you don't have \"64 new 768 neurons\" for each batch. Let's clarify what's happening with the dimensions and the neural network.\n",
    "\n",
    "Your understanding of sentence_embedding is correct: for each sentence in a batch, you extract its 768-dimensional [CLS] token embedding.\n",
    "\n",
    "Here's how it plays out with your classifier:\n",
    "\n",
    "Output Dimensions and Neurons\n",
    "sentence_embedding (Input to self.classifier)\n",
    "\n",
    "For a batch size of 64, sentence_embedding will have a shape of (64, 768).\n",
    "\n",
    "This means you have 64 independent 768-dimensional vectors, one for each sentence in the batch. These are inputs to your classifier.\n",
    "\n",
    "self.classifier Layers\n",
    "\n",
    "nn.Linear(self.bert.config.hidden_size, 256):\n",
    "\n",
    "self.bert.config.hidden_size is 768.\n",
    "\n",
    "This layer takes the (64, 768) sentence_embedding as input.\n",
    "\n",
    "It maps each 768-dimensional input vector to a 256-dimensional output vector.\n",
    "\n",
    "The output shape of this layer will be (64, 256).\n",
    "\n",
    "This layer has 768 input \"neurons\" (or input features) and 256 output \"neurons\" (or output features). The connections between these are learned weights.\n",
    "\n",
    "nn.ReLU(): This is an activation function applied element-wise. It doesn't change the tensor's shape. The shape remains (64, 256).\n",
    "\n",
    "nn.Dropout(0.3): This randomly sets a fraction of input units to zero during training to prevent overfitting. It also doesn't change the tensor's shape. The shape remains (64, 256).\n",
    "\n",
    "nn.Linear(256, 1):\n",
    "\n",
    "This layer takes the (64, 256) tensor from the previous layer.\n",
    "\n",
    "It maps each 256-dimensional input vector to a 1-dimensional output (a single scalar).\n",
    "\n",
    "The output shape of this layer will be (64, 1).\n",
    "\n",
    "This layer has 256 input \"neurons\" and 1 output \"neuron\".\n",
    "\n",
    "nn.Sigmoid(): This activation function squashes the output of the final linear layer to a probability between 0 and 1. It doesn't change the tensor's shape. The final outputs shape is (64, 1).\n",
    "\n",
    "Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3c03238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=SentimentClassifier()\n",
    "criterion=nn.BCELoss()\n",
    "optimizer=AdamW(model.parameters(), lr=0.001)\n",
    "criterion=criterion.to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439ba03",
   "metadata": {},
   "source": [
    "### For a batch size of 64:\n",
    "\n",
    "outputs (before squeeze()): The shape will be (64, 1). Visually, it's more like [[0.7], [0.1], ..., [0.9]]. Each inner list has one probability.\n",
    "\n",
    "labels: The shape will be (64,). Visually, it's like [1, 0, ..., 1]. Each element is a single binary label.\n",
    "\n",
    "After outputs.squeeze(), outputs will become (64,), matching the labels shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d2471d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1, Loss: 0.0763472467660904\n",
      "Epoch: 1, Batch: 2, Loss: 0.02298722416162491\n",
      "Epoch: 1, Batch: 3, Loss: 0.1071750819683075\n",
      "Epoch: 1, Batch: 4, Loss: 0.0867910087108612\n",
      "Epoch: 1, Batch: 5, Loss: 0.06929303705692291\n",
      "Epoch: 1, Batch: 6, Loss: 0.07982479780912399\n",
      "Epoch: 1, Batch: 7, Loss: 0.07815876603126526\n",
      "Epoch: 1, Batch: 8, Loss: 0.19440428912639618\n",
      "Epoch: 1, Batch: 9, Loss: 0.08208862692117691\n",
      "Epoch: 1, Batch: 10, Loss: 0.033225275576114655\n",
      "Epoch: 1, Batch: 11, Loss: 0.0723995566368103\n",
      "Epoch: 1, Batch: 12, Loss: 0.04688543826341629\n",
      "Epoch: 1, Batch: 13, Loss: 0.14591826498508453\n",
      "Epoch: 1, Batch: 14, Loss: 0.1261478066444397\n",
      "Epoch: 1, Batch: 15, Loss: 0.20270179212093353\n",
      "Epoch: 1, Batch: 16, Loss: 0.03362751379609108\n",
      "Epoch: 1, Batch: 17, Loss: 0.11942648142576218\n",
      "Epoch: 1, Batch: 18, Loss: 0.09854297339916229\n",
      "Epoch: 1, Batch: 19, Loss: 0.08300063014030457\n",
      "Epoch: 1, Batch: 20, Loss: 0.14304859936237335\n",
      "Epoch: 1, Batch: 21, Loss: 0.05111919716000557\n",
      "Epoch: 1, Batch: 22, Loss: 0.0910450667142868\n",
      "Epoch: 1, Average Training Loss: 0.09291630339893428\n",
      "Epoch: 2, Batch: 1, Loss: 0.07851698994636536\n",
      "Epoch: 2, Batch: 2, Loss: 0.07679785788059235\n",
      "Epoch: 2, Batch: 3, Loss: 0.12493813782930374\n",
      "Epoch: 2, Batch: 4, Loss: 0.06199583411216736\n",
      "Epoch: 2, Batch: 5, Loss: 0.02649008482694626\n",
      "Epoch: 2, Batch: 6, Loss: 0.06292541325092316\n",
      "Epoch: 2, Batch: 7, Loss: 0.0498700812458992\n",
      "Epoch: 2, Batch: 8, Loss: 0.07651710510253906\n",
      "Epoch: 2, Batch: 9, Loss: 0.028142038732767105\n",
      "Epoch: 2, Batch: 10, Loss: 0.08980042487382889\n",
      "Epoch: 2, Batch: 11, Loss: 0.19091328978538513\n",
      "Epoch: 2, Batch: 12, Loss: 0.09340128302574158\n",
      "Epoch: 2, Batch: 13, Loss: 0.10498221218585968\n",
      "Epoch: 2, Batch: 14, Loss: 0.11234355717897415\n",
      "Epoch: 2, Batch: 15, Loss: 0.0449758917093277\n",
      "Epoch: 2, Batch: 16, Loss: 0.10917891561985016\n",
      "Epoch: 2, Batch: 17, Loss: 0.06341719627380371\n",
      "Epoch: 2, Batch: 18, Loss: 0.0988227054476738\n",
      "Epoch: 2, Batch: 19, Loss: 0.06065135449171066\n",
      "Epoch: 2, Batch: 20, Loss: 0.05329982191324234\n",
      "Epoch: 2, Batch: 21, Loss: 0.040265683084726334\n",
      "Epoch: 2, Batch: 22, Loss: 0.10000825673341751\n",
      "Epoch: 2, Average Training Loss: 0.07946609705686569\n"
     ]
    }
   ],
   "source": [
    "epochs=2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch,(input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        input_ids=input_ids.to(device)\n",
    "        attention_mask=attention_mask.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(input_ids=input_ids, attention_mask=attention_mask)##sending the input_ids and attention_mask to the model to get the outputs of the classifier 0 or 1\n",
    "        loss=criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1}, Batch: {batch+1}, Loss: {loss.item()}\")\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Average Training Loss: {avg_train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "987fc01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 0.06580272937814395\n",
      "Accuracy: 97.71%\n"
     ]
    }
   ],
   "source": [
    "## Evaluation\n",
    "model.eval()\n",
    "total_eval_loss = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (input_ids, attention_mask, labels) in enumerate(test_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        predictions = (outputs.squeeze() > 0.5).float()  # Convert probabilities to binary predictions\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / len(test_dataset)\n",
    "\n",
    "    print(f\"Average Evaluation Loss: {avg_eval_loss}\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "435d41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,text,max_length=128):\n",
    "    tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    #Tokenize the input text\n",
    "    encoding=tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids=encoding['input_ids'].to(device)##basic token ids of the sentence\n",
    "    attention_mask=encoding['attention_mask'].to(device)##attention mask to ignore the padding tokens\n",
    "\n",
    "    model=model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output=model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        prediction=(output.squeeze() > 0.5).float()\n",
    "        return 'spam' if prediction.item() == 1 else 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dec695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"Congratulations! You've won a lottery worth $1,000,000! Claim your prize now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cbc92762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model,\"Hello,you are eligible for a discount on your next purchase.Click here to claim your offer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4bea9970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e77e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
